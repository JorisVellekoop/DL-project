{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B2lV9cUoIqc"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Mar  9 13:55:26 2021\n",
        "\n",
        "@author: remco\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self,input_channels,output_channels):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        \n",
        "        self.VGG16 = models.vgg16(pretrained=True)\n",
        "        \n",
        "        \n",
        "        ###Encoder\n",
        "        #First layer\n",
        "        self.encoder_conv_11    = nn.Conv2d(input_channels,64,kernel_size=3,padding=1)\n",
        "        self.encoder_bn_11      = nn.BatchNorm2d(64)\n",
        "        self.encoder_conv_12    = nn.Conv2d(64,64,kernel_size=3,padding=1)\n",
        "        self.encoder_bn_12      = nn.BatchNorm2d(64)\n",
        "        \n",
        "        #Second layer\n",
        "        self.encoder_conv_21    = nn.Conv2d(64,128,kernel_size=3,padding=1)\n",
        "        self.encoder_bn_21      = nn.BatchNorm2d(128)\n",
        "        self.encoder_conv_22    = nn.Conv2d(128,128,kernel_size=3,padding=1)\n",
        "        self.encoder_bn_22      = nn.BatchNorm2d(128) \n",
        "        \n",
        "        #Third layer\n",
        "        self.encoder_conv_31     = nn.Conv2d(128,256,kernel_size=3,padding=1)\n",
        "        self.encoder_bn_31      = nn.BatchNorm2d(256)\n",
        "        self.encoder_conv_32    = nn.Conv2d(256,256,kernel_size=3,padding=1)\n",
        "        self.encoder_bn_32      = nn.BatchNorm2d(256) \n",
        "        self.encoder_conv_33    = nn.Conv2d(256,256,kernel_size=3,padding=1)\n",
        "        self.encoder_bn_33      = nn.BatchNorm2d(256) \n",
        "        \n",
        "        #Fourth layer\n",
        "        self.encoder_conv_41    = nn.Conv2d(256,512,kernel_size=3,padding=1)\n",
        "        self.encoder_bn_41      = nn.BatchNorm2d(512)\n",
        "        self.encoder_conv_42    = nn.Conv2d(512,5126,kernel_size=3,padding=1)\n",
        "        self.encoder_bn_42      = nn.BatchNorm2d(512) \n",
        "        self.encoder_conv_43    = nn.Conv2d(512,512,kernel_size=3,padding=1)\n",
        "        self.encoder_bn_43      = nn.BatchNorm2d(512) \n",
        "        \n",
        "        #Fift layer\n",
        "        self.encoder_conv_51    = nn.Conv2d(512,512,kernel_size=3,padding=1)\n",
        "        self.encoder_bn_51      = nn.BatchNorm2d(512)\n",
        "        self.encoder_conv_52    = nn.Conv2d(512,5126,kernel_size=3,padding=1)\n",
        "        self.encoder_bn_52      = nn.BatchNorm2d(512) \n",
        "        self.encoder_conv_53    = nn.Conv2d(512,512,kernel_size=3,padding=1)\n",
        "        self.encoder_bn_53      = nn.BatchNorm2d(512) \n",
        "        \n",
        "        self.set_encoder_params()\n",
        "        \n",
        "        ###Decoder\n",
        "        #Fift Layer\n",
        "        self.decoder_conv_53  = nn.ConvTranspose2d(512,512,kernel_size=3,padding=1)\n",
        "        self.decoder_bn_53      = nn.BatchNorm2d(512)\n",
        "        self.decoder_conv_52  = nn.ConvTranspose2d(512,512,kernel_size=3,padding=1)\n",
        "        self.decoder_bn_52      = nn.BatchNorm2d(512)\n",
        "        self.decoder_conv_51  = nn.ConvTranspose2d(512,512,kernel_size=3,padding=1)\n",
        "        self.decoder_bn_51      = nn.BatchNorm2d(512)\n",
        "        \n",
        "        #Fourd Layer\n",
        "        self.decoder_conv_43  = nn.ConvTranspose2d(512,512,kernel_size=3,padding=1)\n",
        "        self.decoder_bn_43      = nn.BatchNorm2d(512)\n",
        "        self.decoder_conv_42  = nn.ConvTranspose2d(512,512,kernel_size=3,padding=1)\n",
        "        self.decoder_bn_42      = nn.BatchNorm2d(512)\n",
        "        self.decoder_conv_41  = nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=3,padding=1)\n",
        "        self.decoder_bn_41      = nn.BatchNorm2d(256)\n",
        "        \n",
        "        #Third Layer\n",
        "        self.decoder_conv_33  = nn.ConvTranspose2d(256,256,kernel_size=3,padding=1)\n",
        "        self.decoder_bn_33      = nn.BatchNorm2d(256)\n",
        "        self.decoder_conv_32  = nn.ConvTranspose2d(256,256,kernel_size=3,padding=1)\n",
        "        self.decoder_bn_32      = nn.BatchNorm2d(256)\n",
        "        self.decoder_conv_31  = nn.ConvTranspose2d(256,128,kernel_size=3,padding=1)\n",
        "        self.decoder_bn_31      = nn.BatchNorm2d(128)\n",
        "        \n",
        "        #Second Layer\n",
        "        self.decoder_conv_22  = nn.ConvTranspose2d(128,128,kernel_size=3,padding=1)\n",
        "        self.decoder_bn_22      = nn.BatchNorm2d(128)\n",
        "        self.decoder_conv_21  = nn.ConvTranspose2d(128,64,kernel_size=3,padding=1)\n",
        "        self.decoder_bn_21      = nn.BatchNorm2d(64)\n",
        "        \n",
        "        #First Layer\n",
        "        self.decoder_conv_12  = nn.ConvTranspose2d(64,64,kernel_size=3,padding=1)\n",
        "        self.decoder_bn_12      = nn.BatchNorm2d(64)\n",
        "        self.decoder_conv_11  = nn.ConvTranspose2d(64,output_channels,kernel_size=3,padding=1)\n",
        "        \n",
        "    def set_encoder_params(self):\n",
        "        #First layer\n",
        "        self.encoder_conv_11.weight.data = self.VGG16.features[0].weight.data\n",
        "        self.encoder_conv_11.bias.data = self.VGG16.features[0].bias.data\n",
        "        \n",
        "        self.encoder_conv_12.weight.data = self.VGG16.features[2].weight.data\n",
        "        self.encoder_conv_12.bias.data = self.VGG16.features[2].bias.data\n",
        "        \n",
        "        #Second Layer\n",
        "        self.encoder_conv_21.weight.data = self.VGG16.features[5].weight.data\n",
        "        self.encoder_conv_21.bias.data = self.VGG16.features[5].bias.data\n",
        "        \n",
        "        self.encoder_conv_22.weight.data = self.VGG16.features[7].weight.data\n",
        "        self.encoder_conv_22.bias.data = self.VGG16.features[7].bias.data\n",
        "                \n",
        "        #Third layer\n",
        "        self.encoder_conv_31.weight.data = self.VGG16.features[10].weight.data\n",
        "        self.encoder_conv_31.bias.data = self.VGG16.features[10].bias.data\n",
        "        \n",
        "        self.encoder_conv_32.weight.data = self.VGG16.features[12].weight.data\n",
        "        self.encoder_conv_32.bias.data = self.VGG16.features[12].bias.data\n",
        "        \n",
        "        self.encoder_conv_33.weight.data = self.VGG16.features[14].weight.data\n",
        "        self.encoder_conv_33.bias.data = self.VGG16.features[14].bias.data\n",
        "        \n",
        "        #Fourth Layer\n",
        "        self.encoder_conv_41.weight.data = self.VGG16.features[17].weight.data\n",
        "        self.encoder_conv_41.bias.data = self.VGG16.features[17].bias.data\n",
        "        \n",
        "        self.encoder_conv_42.weight.data = self.VGG16.features[19].weight.data\n",
        "        self.encoder_conv_42.bias.data = self.VGG16.features[19].bias.data\n",
        "        \n",
        "        self.encoder_conv_43.weight.data = self.VGG16.features[21].weight.data\n",
        "        self.encoder_conv_43.bias.data = self.VGG16.features[21].bias.data\n",
        "        \n",
        "        #Fift layer\n",
        "        self.encoder_conv_51.weight.data = self.VGG16.features[24].weight.data\n",
        "        self.encoder_conv_51.bias.data = self.VGG16.features[24].bias.data\n",
        "        \n",
        "        self.encoder_conv_52.weight.data = self.VGG16.features[26].weight.data\n",
        "        self.encoder_conv_52.bias.data = self.VGG16.features[26].bias.data\n",
        "        \n",
        "        self.encoder_conv_53.weight.data = self.VGG16.features[28].weight.data\n",
        "        self.encoder_conv_53.bias.data = self.VGG16.features[28].bias.data\n",
        "        \n",
        "    def forward(self,input_image):\n",
        "        #Encoder\n",
        "        #First Layer\n",
        "        size_1 = input_image.size()\n",
        "        \n",
        "        x = F.relu(self.encoder_bn_11(self.encoder_conv_11(input_image)))\n",
        "        x = F.relu(self.encoder_bn_12(self.encoder_conv_12(x)))\n",
        "        x, idx1 = F.max_pool2d(x,kernel_size=2,stride=2,return_indices=True)\n",
        "        \n",
        "        #Second Layer\n",
        "        size_2 = x.size()\n",
        "        x = F.relu(self.encoder_bn_21(self.encoder_conv_21(x)))\n",
        "        x = F.relu(self.encoder_bn_22(self.encoder_conv_22(x)))\n",
        "        x, idx2 = F.max_pool2d(x,kernel_size=2,stride=2,return_indices=True)\n",
        "        \n",
        "        #Third Layer\n",
        "        size_3 = x.size()\n",
        "        x = F.relu(self.encoder_bn_31(self.encoder_conv_31(x)))\n",
        "        x = F.relu(self.encoder_bn_32(self.encoder_conv_32(x)))\n",
        "        x = F.relu(self.encoder_bn_33(self.encoder_conv_33(x)))\n",
        "        x, idx3 = F.max_pool2d(x,kernel_size=2,stride=2,return_indices=True)\n",
        "        \n",
        "        #Fourth Layer\n",
        "        size_4 = x.size()\n",
        "        x = F.relu(self.encoder_bn_41(self.encoder_conv_41(x)))\n",
        "        x = F.relu(self.encoder_bn_42(self.encoder_conv_42(x)))\n",
        "        x = F.relu(self.encoder_bn_43(self.encoder_conv_43(x)))\n",
        "        x, idx4 = F.max_pool2d(x,kernel_size=2,stride=2,return_indices=True)\n",
        "        \n",
        "        #Fifth layer\n",
        "        size_5 = x.size()\n",
        "        x = F.relu(self.encoder_bn_51(self.encoder_conv_51(x)))\n",
        "        x = F.relu(self.encoder_bn_52(self.encoder_conv_52(x)))\n",
        "        x = F.relu(self.encoder_bn_53(self.encoder_conv_53(x)))\n",
        "        x, idx5 = F.max_pool2d(x,kernel_size=2,stride=2,return_indices=True)\n",
        "        \n",
        "        #Decoder\n",
        "        #Fifth Layer\n",
        "        x = F.max_unpool2d(x, idx5, kernel_size=2,stride=2,output_size= size_5)\n",
        "        x = F.relu(self.decoder_bn_53(self.decoder_conv_53(x)))\n",
        "        x = F.relu(self.decoder_bn_52(self.decoder_conv_52(x)))\n",
        "        x = F.relu(self.decoder_bn_51(self.decoder_conv_51(x)))\n",
        "        \n",
        "        #Fourth Layer\n",
        "        x = F.max_unpool2d(x, idx4, kernel_size=2,stride=2,output_size= size_4)\n",
        "        x = F.relu(self.decoder_bn_43(self.decoder_conv_43(x)))\n",
        "        x = F.relu(self.decoder_bn_42(self.decoder_conv_42(x)))\n",
        "        x = F.relu(self.decoder_bn_41(self.decoder_conv_41(x)))\n",
        "        \n",
        "        \n",
        "        #Third Layer\n",
        "        x = F.max_unpool2d(x, idx3, kernel_size=2,stride=2,output_size= size_3)\n",
        "        x = F.relu(self.decoder_bn_33(self.decoder_conv_33(x)))\n",
        "        x = F.relu(self.decoder_bn_32(self.decoder_conv_32(x)))\n",
        "        x = F.relu(self.decoder_bn_31(self.decoder_conv_31(x)))\n",
        "                   \n",
        "        #Second Layer\n",
        "        x = F.max_unpool2d(x, idx2, kernel_size=2,stride=2,output_size= size_2)\n",
        "        x = F.relu(self.decoder_bn_22(self.decoder_conv_22(x)))\n",
        "        x = F.relu(self.decoder_bn_21(self.decoder_conv_21(x)))\n",
        "        \n",
        "        #First Layer\n",
        "        x = F.max_unpool2d(x, idx1, kernel_size=2,stride=2,output_size= size_1)\n",
        "        x = F.relu(self.decoder_bn_12(self.decoder_conv_12(x)))\n",
        "        x = self.decoder_conv_11(x)\n",
        "        x_softmax = F.softmax(x,dim=1)\n",
        "        return x, x_softmax\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjWoKaxwndY0"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Mar 24 10:58:21 2021\n",
        "\n",
        "@author: remco\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "from collections import namedtuple\n",
        "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class CamVid(VisionDataset):\n",
        "    \"\"\"`Cityscapes <http://www.cityscapes-dataset.com/>`_ Dataset.\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory ``leftImg8bit``\n",
        "            and ``gtFine`` or ``gtCoarse`` are located.\n",
        "        split (string, optional): The image split to use, ``train``, ``test`` or ``val`` if mode=\"fine\"\n",
        "            otherwise ``train``, ``train_extra`` or ``val``\n",
        "        mode (string, optional): The quality mode to use, ``fine`` or ``coarse``\n",
        "        target_type (string or list, optional): Type of target to use, ``instance``, ``semantic``, ``polygon``\n",
        "            or ``color``. Can also be a list to output a tuple with all specified target types.\n",
        "        transform (callable, optional): A function/transform that takes in a PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
        "            and returns a transformed version.\n",
        "    Examples:\n",
        "        Get semantic segmentation target\n",
        "        .. code-block:: python\n",
        "            dataset = Cityscapes('./data/cityscapes', split='train', mode='fine',\n",
        "                                 target_type='semantic')\n",
        "            img, smnt = dataset[0]\n",
        "        Get multiple targets\n",
        "        .. code-block:: python\n",
        "            dataset = Cityscapes('./data/cityscapes', split='train', mode='fine',\n",
        "                                 target_type=['instance', 'color', 'polygon'])\n",
        "            img, (inst, col, poly) = dataset[0]\n",
        "        Validate on the \"coarse\" set\n",
        "        .. code-block:: python\n",
        "            dataset = Cityscapes('./data/cityscapes', split='val', mode='coarse',\n",
        "                                 target_type='semantic')\n",
        "            img, smnt = dataset[0]\n",
        "    \"\"\"\n",
        "\n",
        "    # Based on https://github.com/mcordts/cityscapesScripts\n",
        "    CamVidClass = namedtuple('Class', ['name', 'id', 'train_id', 'category', 'category_id',\n",
        "                                                     'has_instances', 'ignore_in_eval', 'color'])\n",
        "\n",
        "    classes = [\n",
        "        CamVidClass('Void', 0, 255, 'void', 0, False, True, (0, 0, 0)),\n",
        "        \n",
        "        CamVidClass('Archway', 2, 255, 'void', 0, True, True, (192, 0, 128)),\n",
        "        CamVidClass('Animal', 1, 255, 'void', 0, True, True, (64, 128, 64)),\n",
        "        CamVidClass('Bicyclist', 3, 0, 'Bicyclist', 1, True, False, (0, 128, 192)),  #\n",
        "        CamVidClass('Bridge', 4, 255, 'void', 0, True, True, (0, 128, 64)),\n",
        "        CamVidClass('Building', 5, 1, 'Building', 2, True, False, (128,0,0)), #\n",
        "        CamVidClass('Car', 6, 2, 'Car', 3, True, False, (64,0,128)), #\n",
        "        CamVidClass('CartLuggagePram', 7, 255, 'void', 0, False, True, (64,0,192)),\n",
        "        CamVidClass('Child', 8, 255, 'void', 0, False, True, (192,128,64)),\n",
        "        CamVidClass('Column_pole', 9, 3, 'Pole', 4, True, False, (192,192,128)), #\n",
        "        CamVidClass('Fence', 10, 4, 'Fence', 5, True, False, (64,64,128)), #\n",
        "        CamVidClass('LaneMkgsDriv', 11, 255, 'void', 0, True, True, (128,0,192)),\n",
        "        CamVidClass('LaneMkgsNonDriv', 12, 255, 'void', 0, True, True, (192,0,64)),\n",
        "        CamVidClass('Misc_Text', 13, 255, 'void', 0, False, True, (128,128,64)),\n",
        "        CamVidClass('MotorcycleScooter', 14, 255, 'void', 0, True, True, (192,0,192)),\n",
        "        CamVidClass('OtherMoving', 15, 255, 'void', 0, True, True, (128,64,64)),\n",
        "        CamVidClass('ParkingBlock', 16, 255, 'void', 0, True, True, (64,192,128)),\n",
        "        CamVidClass('Pedestrian', 17, 5, 'Pedestrian', 6, True, False,  (64,64,0)), #\n",
        "        CamVidClass('Road', 18, 6, 'Road', 7, True, False, (128,64,128)), #\n",
        "        CamVidClass('RoadShoulder', 19, 255, 'void', 0, False, True, (128,128,192)),\n",
        "        CamVidClass('Sidewalk', 20, 7, 'Sidewalk', 8, True, False, (0,0,192)), #\n",
        "        CamVidClass('SignSymbol', 21, 8, 'SignSymbol', 9, True, False, (192,128,128)), #\n",
        "        CamVidClass('Sky', 22, 9, 'Sky', 10, True, False, (128,128,128)), #\n",
        "        CamVidClass('SUVPickupTruck', 23, 255, 'void', 0, False, True, (64,128,192)),\n",
        "        CamVidClass('TrafficCone', 24, 255, 'void', 0, True, True, (0,0,64)),\n",
        "        CamVidClass('TrafficLight', 25, 255, 'void', 0, True, True, (0,64,64)),\n",
        "        CamVidClass('Train', 26, 255, 'void', 0, True, True, (192,64,128)),\n",
        "        CamVidClass('Tree', 27, 10, 'Tree', 11, True, False, (128,128,0)), #\n",
        "        CamVidClass('Truck_Bus', 28, 255, 'void', 0, True, True, (192,128,192)),\n",
        "        CamVidClass('Tunnel', 29, 255, 'void', 0, True, True, (64,0,64)),\n",
        "        CamVidClass('VegetationMisc', 30, 255, 'void', 0, True, True, (192,192,0)),\n",
        "        CamVidClass('Wall', 31, 255, 'void', 0, True, True, (64,192,0)),\n",
        "        \n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            root: str,\n",
        "            split: str = \"train\",\n",
        "            target_type: Union[List[str], str] = \"instance\",\n",
        "            transform: Optional[Callable] = None,\n",
        "            target_transform: Optional[Callable] = None,\n",
        "            transforms: Optional[Callable] = None,\n",
        "    ) -> None:\n",
        "        super(CamVid, self).__init__(root, transforms, transform, target_transform)\n",
        "        self.images_dir = os.path.join(self.root, split, 'images')\n",
        "        self.targets_dir = os.path.join(self.root, split, 'labels')\n",
        "        self.target_type = target_type\n",
        "        self.split = split\n",
        "        self.images = []\n",
        "        self.targets = []\n",
        "        self.colours = []\n",
        "        self.colour_names = []\n",
        "        self.only_colours = []\n",
        "        self.correct_classes = 0\n",
        "        self.ignore_classes= 0\n",
        "        \n",
        "        for i in range(len(self.classes)):\n",
        "            if self.classes[i][2] != 255:\n",
        "                self.colours.append((self.classes[i][0],self.classes[i][7]))\n",
        "                self.only_colours.append(self.classes[i][7])\n",
        "                self.correct_classes += 1\n",
        "                self.colour_names.append(self.classes[i][0])\n",
        "\n",
        "        for i in range(len(self.classes)):\n",
        "            if self.classes[i][2] == 255:\n",
        "                self.colours.append((self.classes[i][0],self.classes[i][7]))\n",
        "        \n",
        "        for file_name in os.listdir(self.images_dir):\n",
        "                #target_types = []\n",
        "                #for t in self.target_type:\n",
        "                target_name = (os.path.splitext(file_name)[0])\n",
        "                new_name =\"_\".join([target_name,\"L.png\"])\n",
        "                target_types = (os.path.join(self.targets_dir, new_name))\n",
        "                    \n",
        "                #print(target_types)\n",
        "\n",
        "                self.images.append(os.path.join(self.images_dir, file_name))\n",
        "                self.targets.append(target_types)\n",
        "\n",
        "        self.NUM_CLASSES = len(self.classes)\n",
        "        #only in training set\n",
        "        if split == \"train\":\n",
        "           self.counts = self.__compute_class_probability()    \n",
        "        if split == \"train_small\":\n",
        "           self.counts = self.__compute_class_probability()     \n",
        "\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is a tuple of all target types if target_type is a list with more\n",
        "            than one item. Otherwise target is a json object if target_type=\"polygon\", else the image segmentation.\n",
        "        \"\"\"\n",
        "\n",
        "        image = Image.open(self.images[index]).convert('RGB')\n",
        "\n",
        "        targets: Any = []\n",
        "        target = Image.open(self.targets[index])\n",
        "\n",
        "        targets.append(target)\n",
        "\n",
        "        target = tuple(targets) if len(targets) > 1 else targets[0]\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            image, target = self.transforms(image, target)\n",
        "        \n",
        "            \n",
        "        target = self.make_mask(target)\n",
        "        image = torchvision.transforms.functional.to_tensor(image)\n",
        "        target = torchvision.transforms.functional.to_tensor(target)\n",
        "        \n",
        "        image = self.LocalContrastNorm(image,9) #if rgb image is used\n",
        "        #image = Transform_Maddern(image,HS=True)  #Maddern (if HS=True: Hue Saturated)\n",
        "        \n",
        "        return image, target\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        lines = [\"Split: {split}\", \"Mode: {mode}\", \"Type: {target_type}\"]\n",
        "        return '\\n'.join(lines).format(**self.__dict__)\n",
        "\n",
        "\n",
        "    def _get_target_suffix(self, target_type: str) -> str:\n",
        "        if target_type == 'instance':\n",
        "            return '{}_instanceIds.png'\n",
        "        elif target_type == 'semantic':\n",
        "            return '{}_labelIds.png'\n",
        "        elif target_type == 'color':\n",
        "            return '{}_color.png'\n",
        "        else:\n",
        "            return '{}_polygons.json'\n",
        "\n",
        "   \n",
        "    def LocalContrastNorm(self, image, radius=9):\n",
        "      \"\"\"\n",
        "      image: torch.Tensor , .shape => (1,channels,height,width)\n",
        "\n",
        "        radius: Gaussian filter size (int), odd\n",
        "      \"\"\"\n",
        "      if radius % 2 == 0:\n",
        "          radius += 1\n",
        "\n",
        "      def get_gaussian_filter(kernel_shape):\n",
        "          x = np.zeros(kernel_shape, dtype='float64')\n",
        "\n",
        "          def gauss(x, y, sigma=2.0):\n",
        "              Z = 2 * np.pi * sigma ** 2\n",
        "              return 1. / Z * np.exp(-(x * 2 + y * 2) / (2. * sigma ** 2))\n",
        "\n",
        "          mid = np.floor(kernel_shape[-1] / 2.)\n",
        "          for kernel_idx in range(0, kernel_shape[1]):\n",
        "              for i in range(0, kernel_shape[2]):\n",
        "                  for j in range(0, kernel_shape[3]):\n",
        "                      x[0, kernel_idx, i, j] = gauss(i - mid, j - mid)\n",
        "\n",
        "          return x / np.sum(x)\n",
        "      \n",
        "      n, c, h, w = 1, image.shape[0], image.shape[1], image.shape[2]\n",
        "      image_correct = torch.Tensor(n,c,h,w)\n",
        "      image_correct[0,:,:,:] = image\n",
        "      gaussian_filter = torch.Tensor(get_gaussian_filter((1, c, radius, radius)))\n",
        "      filtered_out = torch.nn.functional.conv2d(image_correct, gaussian_filter, padding=radius - 1)\n",
        "      mid = int(np.floor(gaussian_filter.shape[2] / 2.))\n",
        "      ### Subtractive Normalization\n",
        "      centered_image = image_correct - filtered_out[:, :, mid:-mid, mid:-mid]\n",
        "\n",
        "      ## Variance Calc\n",
        "      sum_sqr_image = torch.nn.functional.conv2d(centered_image.pow(2), gaussian_filter, padding=radius - 1)\n",
        "      s_deviation = sum_sqr_image[:, :, mid:-mid, mid:-mid].sqrt()\n",
        "      per_img_mean = s_deviation.mean()\n",
        "\n",
        "      ## Divisive Normalization\n",
        "      divisor = np.maximum(per_img_mean.numpy(), s_deviation.numpy())\n",
        "      divisor = np.maximum(divisor, 1e-4)\n",
        "      new_image = centered_image / torch.Tensor(divisor)\n",
        "      return new_image[0,:,:,:]\n",
        "    \n",
        "    def __compute_class_probability(self):\n",
        "        counts = dict((i, 0) for i in range(self.correct_classes+1))\n",
        "        for name in self.images:\n",
        "            first_partname = name.split('.')[0]\n",
        "            name_2 = first_partname.split( '/' )[-1]\n",
        "            mask_path = os.path.join(self.targets_dir, \"_\".join([name_2,\"L.png\"]))\n",
        "            raw_image = Image.open(mask_path).resize((360, 480))\n",
        "            #raw_image = torchvision.transforms.functional.to_tensor(raw_image)\n",
        "            masked = self.make_mask(raw_image).transpose(2,0,1)\n",
        "            masked = torchvision.transforms.functional.to_tensor(masked)\n",
        "            masked = torch.max(masked,1)[1]\n",
        "            #print(masked.shape)\n",
        "            imx_t = np.array(masked).reshape(360* 480)\n",
        "            imx_t[imx_t==255] = len(self.classes)\n",
        "\n",
        "            for i in range(self.correct_classes+1):\n",
        "                counts[i] += np.sum(imx_t == i)\n",
        "        #counts[counts== np.inf] = 0\n",
        "        return counts\n",
        "\n",
        "    def get_class_probability(self):\n",
        "        values = np.array(list(self.counts.values()))\n",
        "        p_values = values/np.sum(values)\n",
        "        p_values[p_values== -np.inf] = 0\n",
        "        return torch.Tensor(p_values)\n",
        "        \n",
        "    def make_mask(self,mask):\n",
        "          semantic_map = []\n",
        "          for colour in self.colours:  \n",
        "            equality = np.equal(mask, colour[1])\n",
        "            class_map = np.all(equality, axis = -1)\n",
        "            semantic_map.append(class_map)\n",
        "          semantic_map = np.stack(semantic_map, axis=-1)\n",
        "        \n",
        "          return semantic_map\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKnYRnGrluWh"
      },
      "source": [
        "def Transform_Maddern(image, a=0.48, HS=False):\n",
        "    I_maddern_first = 0.5* torch.ones(image[0].shape)\n",
        "    I_maddern_first[image[1]>0] += torch.log(image[1][image[1]>0])\n",
        "    I_maddern_first[image[2]>0] += - a * torch.log(image[2][image[2]>0])\n",
        "    I_maddern_first[image[0]>0] += - (1 - a) * torch.log(image[0][image[0]>0])\n",
        "\n",
        "    if HS:\n",
        "        hue, sat, _ = RGB2HSV(image)\n",
        "        I_maddern = torch.dstack((I_maddern_first, hue, sat))\n",
        "        I_maddern = I_maddern.permute(2,0,1)\n",
        "    else:\n",
        "      I_maddern = torch.ones(3,image[0].shape[0],image[0].shape[1])\n",
        "      I_maddern[0,:,:] = I_maddern_first\n",
        "      I_maddern[1,:,:] = torch.zeros(image[0].shape)\n",
        "      I_maddern[2,:,:] = torch.zeros(image[0].shape)\n",
        "\n",
        "\n",
        "    return I_maddern\n",
        "\n",
        "def RGB2HSV(image):\n",
        "    max = torch.max(image, dim=0)\n",
        "    min = torch.min(image, dim=0)\n",
        "    range = max.values-min.values\n",
        "\n",
        "    hue, sat = torch.zeros(image[0].shape), torch.zeros(image[0].shape)\n",
        "\n",
        "    idx = (max.indices == 0) & (range != 0)\n",
        "    hue[idx] = (image[1][idx] - image[2][idx])/(range[idx]) % 6\n",
        "    idx = (max.indices == 1) & (range != 0)\n",
        "    hue[idx] = 2 + (image[2][idx] - image[0][idx])/(range[idx])\n",
        "    idx = (max.indices == 2) & (range != 0)\n",
        "    hue[idx] = 4 + (image[0][idx] - image[1][idx])/(range[idx])\n",
        "    # If range = 0, hue = 0\n",
        "    #heb niet het idee dat heb probleem\n",
        "    idx = max.values != 0\n",
        "    sat[idx] = range[idx]/max.values[idx]\n",
        "    # if max = 0, sat = 0\n",
        "\n",
        "    return 60*hue, sat, max.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Py_LcustPI7"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import os\n",
        "import time \n",
        "from google.colab import drive\n",
        "\n",
        "read_path = '/content/drive/MyDrive/ColabNotebooks/ProjectDL/CamVidData_2'\n",
        "\n",
        "batch_size = 12\n",
        "num_epoch = 150\n",
        "\n",
        "USE_GPU = True\n",
        "\n",
        "GPU_ID = 0\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((360,480)), \n",
        "     ])\n",
        "\n",
        "target_transform = transforms.Compose(\n",
        "    [transforms.Resize((360,480)),\n",
        "     ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsIIeTNCoAL9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "ec33d68e-9cac-43e1-e7ed-10f4c1892b3f"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Mar 24 15:46:21 2021\n",
        "\n",
        "@author: remco\n",
        "\"\"\"\n",
        "\n",
        "        \n",
        "camvid_train = CamVid(read_path,'train',transform=transform,target_transform=target_transform)\n",
        "camvid_val = CamVid(read_path,'val',transform=transform,target_transform=target_transform)\n",
        "\n",
        "\n",
        "\n",
        "dataloader_train = DataLoader(camvid_train, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val = DataLoader(camvid_val, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class_weights = 1.0/camvid_train.get_class_probability()\n",
        "n_classes = len(camvid_train.colours)\n",
        "\n",
        "if USE_GPU == True:\n",
        "  model = CNN(3,camvid_train.correct_classes+1).cuda(GPU_ID)\n",
        "  criterion = nn.CrossEntropyLoss(weight=class_weights,ignore_index=12).cuda(GPU_ID)\n",
        "else:\n",
        "  model = CNN(3,camvid_train.correct_classes+1)\n",
        "  criterion = nn.CrossEntropyLoss(weight=class_weights,ignore_index=12)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5*10**(-4))\n",
        "\n",
        "\n",
        "prev_loss = float('inf')\n",
        "\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    model.train()\n",
        "    t_start = time.time()\n",
        "    loss_val = 0\n",
        "    for i, img_num in enumerate(dataloader_train, 0):\n",
        "      images,labels = img_num\n",
        "      if USE_GPU == True:\n",
        "          images = images.cuda(GPU_ID)\n",
        "          labels = labels.cuda(GPU_ID)\n",
        "      #loss = 0.0\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      predicted, softmaxed = model(images)\n",
        "      labels = torch.clip(torch.max(labels,1)[1],0,camvid_train.correct_classes+1)\n",
        "      loss = criterion(predicted,labels)\n",
        "      loss.backward()\n",
        "      optimizer.step() \n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.eval()  \n",
        "      for i, img_num in enumerate(dataloader_val,0):\n",
        "        images ,labels = img_num\n",
        "        if USE_GPU == True:\n",
        "            images = images.cuda(GPU_ID)\n",
        "            labels = labels.cuda(GPU_ID)\n",
        "        predicted, softmaxed = model(images)\n",
        "        labels = torch.clip(torch.max(labels,1)[1],0,camvid_train.correct_classes+1) #to make all the classes above the correct classes be ignored with the ignore index\n",
        "        los_enumerate = criterion(predicted,labels)\n",
        "        loss_val += los_enumerate.item()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        \n",
        "    delta = time.time() - t_start\n",
        "    print(\"Epoch #{}\\tvalidationLoss: {:.8f}\\t Time: {:2f}s\".format(epoch+1, loss_val\n",
        "                                                          , delta))\n",
        "\n",
        "    if loss_val < prev_loss:\n",
        "      print('#', epoch+1 , \" is currently the best epoch on evaluation data\")\n",
        "      prev_loss = loss_val\n",
        "      model_save_name = 'classifier_maddern_test.pt'\n",
        "      path = F\"/content/drive/MyDrive/ColabNotebooks/ProjectDL/CamVidData_2/{model_save_name}\" \n",
        "      torch.save(model.state_dict(), path)\n",
        "    \n",
        "print(\"finished\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([   7.3657,    4.4705,   31.8390,  884.2386,   75.8206,  284.2722,\n",
            "           3.7967,   16.4691, 1546.2291,    7.0343,   10.0539,   40.1144])\n",
            "Epoch #1\tvalidationLoss: 23.82055593\t Time: 445.504700s\n",
            "# 1  is currently the best epoch on evaluation data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0768ce740ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mt_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m       \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mUSE_GPU\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-d5cd6b84ffe8>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-d5cd6b84ffe8>\u001b[0m in \u001b[0;36mmake_mask\u001b[0;34m(self, mask)\u001b[0m\n\u001b[1;32m    269\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mcolour\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolours\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mequality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mclass_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m             \u001b[0msemantic_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m           \u001b[0msemantic_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemantic_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mall\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mall\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2410\u001b[0m     \"\"\"\n\u001b[0;32m-> 2411\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-oz6-oOcJiD"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "camvid_test = CamVid(read_path,'test',transform=transform,target_transform=target_transform)\n",
        "dataloader_test = DataLoader(camvid_test, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "confmat = np.zeros((camvid_test.correct_classes, camvid_test.correct_classes))\n",
        "\n",
        "only_test = True\n",
        "\n",
        "if only_test:\n",
        "  print(\"loading model\")\n",
        "  model = CNN(3,camvid_test.correct_classes+1).cuda(GPU_ID)\n",
        "  model_save_name = 'classifier_maddern_normal.pt'\n",
        "  path = F\"/content/drive/MyDrive/ColabNotebooks/ProjectDL/CamVidData_2/{model_save_name}\" \n",
        "  model.load_state_dict(torch.load(path))\n",
        "\n",
        "for i, data in enumerate(dataloader_test, 0):\n",
        "    with torch.no_grad():\n",
        "      inputs, labels = data\n",
        "      inputs = inputs.cuda(GPU_ID)\n",
        "      _, softmaxed = model(inputs)\n",
        "      \n",
        "      pred_int = torch.argmax(softmaxed,dim=1)\n",
        "      true_int = torch.clip(torch.max(labels,1)[1],0,camvid_test.correct_classes+1)\n",
        "\n",
        "      conf_new = confusion_matrix(true_int.cpu().data.numpy().flatten(), pred_int.cpu().data.numpy().flatten(),np.arange(11)+1)\n",
        "      confmat += conf_new\n",
        "\n",
        "confmat_chance = confmat / confmat.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.imshow(confmat_chance, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "colour_names = []\n",
        "for i in range(11):\n",
        "  colour_names.append(camvid_test.colours[i][0])\n",
        "\n",
        "plt.xticks(np.arange(11),colour_names, rotation=90)\n",
        "plt.yticks(np.arange(11),colour_names)\n",
        "\n",
        "for i, j in itertools.product(range(confmat_chance.shape[0]), range(confmat_chance.shape[1])):\n",
        "          plt.text(j, i, \"{:0.2f}\".format(confmat_chance[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if confmat_chance[i, j] > 0.5 else \"black\")\n",
        "\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "class_accuracy = np.diagonal(confmat_chance)\n",
        "class_average = np.mean(class_accuracy)\n",
        "global_accuracy = np.trace(confmat)/np.sum(confmat)\n",
        "\n",
        "sum_pred = np.sum(confmat, axis=0)\n",
        "sum_true = np.sum(confmat, axis=1)\n",
        "true_pos = np.diagonal(confmat)\n",
        "\n",
        "precision = np.mean(true_pos /sum_pred)\n",
        "recall = np.mean(true_pos / sum_true)\n",
        "\n",
        "mIoU = np.mean(true_pos/(sum_pred + sum_true - true_pos))\n",
        "\n",
        "print(\"Class accuracy: \")\n",
        "for i in range(11):\n",
        "  print(camvid_test.colours[i][0], \" : \", class_accuracy[i])\n",
        "\n",
        "print(\"Class average accuracy: \", class_average)\n",
        "print(\"global accuracy: \", global_accuracy)\n",
        "print(\"precision: \", precision)\n",
        "print(\"recall: \", recall)\n",
        "print(\"mIoU: \", mIoU)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSUhODZ588qA"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVvsHsL7t5SG"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}